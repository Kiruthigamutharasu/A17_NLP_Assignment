{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2dc3b20",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 17 — TEXT CLEANING, PREPROCESSING & NLP PIPELINE\n",
    "\n",
    "**Dataset**:SMS Spam Collection Dataset  \n",
    "**Source:** https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b216174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d796b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33a71dc",
   "metadata": {},
   "source": [
    "# PART 1 — NLP PIPELINE & BASIC TEXT CLEANING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a78be74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 Samples:\n",
      "\n",
      "  label                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "\n",
      "Text Length:\n",
      "\n",
      "0    111\n",
      "1     29\n",
      "2    155\n",
      "3     49\n",
      "4     61\n",
      "Name: text_length, dtype: int64\n",
      "\n",
      "Example Raw Text Issues:\n",
      "U dun say so early hor... U c already then say...\n"
     ]
    }
   ],
   "source": [
    "# TASK 1: Understanding Raw Text Data\n",
    "# Step 1: Load dataset using pandas\n",
    "df = pd.read_csv(\"spam.csv\", encoding=\"latin-1\")\n",
    "\n",
    "# Keep only useful columns\n",
    "df = df[['v1','v2']]\n",
    "df.columns = ['label','text']\n",
    "\n",
    "# Step 2: Print first 5 samples\n",
    "print(\"\\nFirst 5 Samples:\\n\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 3: Length of each text\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "print(\"\\nText Length:\\n\")\n",
    "print(df['text_length'].head())\n",
    "\n",
    "# Step 4: Identify common issues\n",
    "print(\"\\nExample Raw Text Issues:\")\n",
    "print(df['text'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b4dcfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original vs Basic Cleaned:\n",
      "\n",
      "                                                text  \\\n",
      "0  Go until jurong point, crazy.. Available only ...   \n",
      "1                      Ok lar... Joking wif u oni...   \n",
      "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3  U dun say so early hor... U c already then say...   \n",
      "4  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                    clean_text_basic  \n",
      "0  go until jurong point crazy available only in ...  \n",
      "1                            ok lar joking wif u oni  \n",
      "2  free entry in a wkly comp to win fa cup final ...  \n",
      "3        u dun say so early hor u c already then say  \n",
      "4  nah i dont think he goes to usf he lives aroun...  \n"
     ]
    }
   ],
   "source": [
    "# TASK 2: BASIC TEXT CLEANING\n",
    "# Step 1: Lowercase\n",
    "def basic_clean(text):\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    # Step 2: Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Step 3: Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Step 4: Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Store cleaned text\n",
    "df['clean_text_basic'] = df['text'].apply(basic_clean)\n",
    "\n",
    "print(\"\\nOriginal vs Basic Cleaned:\\n\")\n",
    "print(df[['text','clean_text_basic']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4bbb73",
   "metadata": {},
   "source": [
    "# PART 2 — ADVANCED TEXT CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3eb966e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 3: Removing Noise\n",
    "def advanced_clean(text):\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "    # Remove emails\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # Remove special characters & emojis\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "df['clean_text_advanced'] = df['clean_text_basic'].apply(advanced_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1e2b21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 4: Handling Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered = [w for w in words if w not in stop_words]\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "df['text_no_stopwords'] = df['clean_text_advanced'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8429028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 5: Repeated Characters & Slang (Optional)\n",
    "# Normalize repeated characters\n",
    "def normalize_repeated(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
    "\n",
    "# Slang dictionary\n",
    "slang_dict = {\n",
    "    \"u\":\"you\",\n",
    "    \"gr8\":\"great\",\n",
    "    \"luv\":\"love\"\n",
    "}\n",
    "\n",
    "def replace_slang(text):\n",
    "    words = text.split()\n",
    "    words = [slang_dict.get(w, w) for w in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "df['text_slang_fixed'] = df['text_no_stopwords'].apply(normalize_repeated)\n",
    "df['text_slang_fixed'] = df['text_slang_fixed'].apply(replace_slang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9b4002",
   "metadata": {},
   "source": [
    "# PART 3 — BASIC TEXT PREPROCESSING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f431b645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Tokens Example:\n",
      "['go', 'jurong', 'point', 'crazy', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amore', 'wat']\n",
      "\n",
      "Sentence Tokens Example:\n",
      "['Go until jurong point, crazy..', 'Available only in bugis n great world la e buffet... Cine there got amore wat...']\n"
     ]
    }
   ],
   "source": [
    "# TASK 6: Tokenization\n",
    "print(\"\\nWord Tokens Example:\")\n",
    "print(word_tokenize(df['text_slang_fixed'][0]))\n",
    "\n",
    "print(\"\\nSentence Tokens Example:\")\n",
    "print(sent_tokenize(df['text'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f5f9ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemming Example:\n",
      "['go', 'jurong', 'point', 'crazi', 'avail', 'bugi', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amor', 'wat']\n"
     ]
    }
   ],
   "source": [
    "# TASK 7: Stemming\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "    return [stemmer.stem(word) for word in text.split()]\n",
    "\n",
    "print(\"\\nStemming Example:\")\n",
    "print(stemming(df['text_slang_fixed'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76ac76f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatization Example:\n",
      "['go', 'jurong', 'point', 'crazy', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amore', 'wat']\n"
     ]
    }
   ],
   "source": [
    "# TASK 8: Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    return [lemmatizer.lemmatize(word) for word in text.split()]\n",
    "\n",
    "print(\"\\nLemmatization Example:\")\n",
    "print(lemmatize(df['text_slang_fixed'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "943e1cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Cleaned Text:\n",
      "                                                text  \\\n",
      "0  Go until jurong point, crazy.. Available only ...   \n",
      "1                      Ok lar... Joking wif u oni...   \n",
      "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3  U dun say so early hor... U c already then say...   \n",
      "4  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                    final_clean_text  \n",
      "0  go jurong point crazy available bugis n great ...  \n",
      "1                            ok lar joking wif u oni  \n",
      "2  free entry 2 wkly comp win fa cup final tkts 2...  \n",
      "3                u dun say early hor u c already say  \n",
      "4         nah do not think go usf life around though  \n"
     ]
    }
   ],
   "source": [
    "# TASK 9: FINAL NLP PIPELINE FUNCTION\n",
    "def nlp_preprocess(text):\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Noise removal\n",
    "    text = advanced_clean(text)\n",
    "\n",
    "    # Stopword removal\n",
    "    text = remove_stopwords(text)\n",
    "\n",
    "    # Lemmatization using spaCy\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['final_clean_text'] = df['text'].apply(nlp_preprocess)\n",
    "\n",
    "print(\"\\nFinal Cleaned Text:\")\n",
    "print(df[['text','final_clean_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd9f6f1",
   "metadata": {},
   "source": [
    "##  Task 10 — Observations & Insights\n",
    "\n",
    "###  1. Difference Between Basic and Advanced Cleaning\n",
    "\n",
    "**Basic cleaning** focuses on standardizing text by performing simple operations such as converting text to lowercase, removing punctuation, numbers, and extra spaces. It mainly improves consistency in the dataset.\n",
    "\n",
    "**Advanced cleaning** removes complex noise found in real-world text, such as URLs, email addresses, HTML tags, special characters, emojis, and slang. This step improves the overall quality and usefulness of the text for NLP tasks.\n",
    "\n",
    "\n",
    "\n",
    "###  2. Why Lemmatization is Preferred Over Stemming\n",
    "\n",
    "**Stemming** reduces words by cutting suffixes using rules, which may produce incomplete or incorrect words (e.g., *studies → studi*).\n",
    "\n",
    "**Lemmatization** converts words into their meaningful base form using linguistic knowledge (e.g., *studies → study*).\n",
    "It preserves meaning better and therefore gives more accurate results in NLP applications.\n",
    "\n",
    "\n",
    "\n",
    "### 3. Importance of Preprocessing in NLP Models\n",
    "\n",
    "Text preprocessing is essential because raw text contains noise and inconsistencies. Cleaning and preprocessing:\n",
    "\n",
    "* Remove irrelevant information\n",
    "* Standardize text format\n",
    "* Reduce vocabulary size\n",
    "* Help NLP models understand meaningful patterns\n",
    "\n",
    "Proper preprocessing improves model performance and efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
